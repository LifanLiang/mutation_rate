---
title: "Mutation type specific overdispersion"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: false
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(RColorBrewer)
```

### Problem overview
For the $i$th window with a certain size in the genome, we assume the observed mutation rates ($y_i$) and expected mutation rates ($mu_i$) were generated with the probabilistic model below:
$$
y_i \sim Poisson(\mu_i\theta_i)
$$
$$
\theta_i \sim Gamma(\alpha,\alpha) 
$$


### MLE of overdispersion ($\alpha$)
Integrate out $\theta_i$
$$
P(y_i | \mu_i, \alpha) = \int_0^{+\infty}  P(y_i|\mu_i,\theta_i)P(\theta_i|\alpha) \,d\theta_i
=\dfrac{\mu_i^{y_i} \alpha^\alpha}{y_i!\Gamma(\alpha)}\int_0^{+\infty} \theta_i^{y_i+\alpha-1}e^{-(\mu_i+\alpha)\theta_i}\,d\theta_i
$$
The integral part is the same as the kernel of $\theta_i\sim Gamma(y_i+\alpha, \mu_i+\alpha)$. Thus we got
$$
P(y_i | \mu_i, \alpha)=\dfrac{\mu_i^{y_i} \alpha^\alpha}{y_i!\Gamma(\alpha)}\cdot \dfrac{\Gamma(y_i+\alpha)}{(\mu_i+\alpha)^{y_i+\alpha}}
$$
The loglikelihood would be:
$$
\ell(\alpha;y_i,\mu_i) \propto log(\dfrac{\Gamma(y_i+\alpha)}{\Gamma(\alpha)})-y_ilog(\mu_i+\alpha)-\alpha log(1+\mu_i/\alpha)
$$
Considering the sparsity and the large sample size (3e10/window_size), we can further simplify the computation by isolating the scenario where $y_i=0$.
When $y_i=0,\mu_i=0$, $LL$ can be ignored:
$$
\ell(\alpha;y_i,\mu_i) = const
$$
When $y_i=0,\mu_i>0$:
$$
\ell(\alpha;y_i,\mu_i) \propto -\alpha log(1+\mu_i/\alpha)
$$

### Validation of the estimator
optim with "BFGS" method was used for MLE of overdispersion. I recomputed $\alpha$ for 5k window mutation rates. Estimated $\alpha$ is 2.709, close to the values in the proposal.

### Asymptotic normality of MLE
The second derivative of $\ell(\alpha;y_i,\mu_i)$ is
$$
\frac{d^2}{d\alpha^2}\ell(\alpha;y_i,\mu_i)=\sum_{k=1}^{y_i}\frac{-1}{(\alpha+k)^2}+\frac{y_i}{(\mu_i+\alpha)^2}+\frac{1}{\alpha}-\frac{\mu_i}{(\alpha+\mu_i)^2}-\frac{1}{\alpha+\mu_i}
$$
We used $\frac{d^2}{d\alpha^2}\ell(\alpha;y_i,\mu_i)$ to estimate the standard deviation of $\alpha$.

### Estimating $\alpha$ for different mutation types
* Results here are based on _100 bp_ window size
```{r, echo=F}
alpha.100 <- read.table("data/mute_type_100_alpha.txt", header=T)
knitr::kable(alpha.100)
```

The plot of the gamma distribution. "C_to_A.CpG" and "C_to_G.CpG" are too close to seperate.
```{r, echo=F}
gamma.pdf <- data.frame(x=1:300/100)
plot(NULL, xlim=c(0,2), ylim=c(0,6), ylab="pdf", xlab="theta")
for (i in 1:nrow(alpha.100)) {
  gamma.pdf[alpha.100[i,1]] <- dgamma(gamma.pdf$x, alpha.100[i,2], alpha.100[i,2])
  lines(gamma.pdf$x, gamma.pdf[alpha.100[i,1]][[1]], col=i)
}
legend(1.25,6, legend = alpha.100[,1], col=1:9, lty=1)
```

### Aggregate 100 bp to estimate $\alpha$ for 5K bps
```{r, echo=F}
alpha.5k <- read.table("data/mute_type_5k_alpha.txt", header=T)
ind.A <- startsWith(alpha.5k$mut_type, "A")
ind.C <- !ind.A
alpha.5k$mut_type[ind.A] <- substr(alpha.5k$mut_type[ind.A], 1, unlist(gregexec("_and",alpha.5k$mut_type[ind.A]))-1)
alpha.5k$mut_type[ind.C] <- substr(alpha.5k$mut_type[ind.C], 1, unlist(gregexec("_5k",alpha.5k$mut_type[ind.C]))-1)
knitr::kable(alpha.5k, digits=2)
```

```{r, echo=F}
gamma.pdf <- data.frame(x=1:300/100)
plot(NULL, xlim=c(0,3), ylim=c(0,4), ylab="pdf", xlab="theta")
for (i in 1:nrow(alpha.5k)) {
  gamma.pdf[alpha.5k[i,1]] <- dgamma(gamma.pdf$x, alpha.5k[i,2], alpha.5k[i,2])
  lines(gamma.pdf$x, gamma.pdf[alpha.5k[i,1]][[1]], col=i)
}
legend(1.7, 4, legend = alpha.5k[,1], col=1:9, lty=1)
```

### Observed rates vs. expected rates
#### Correlation at 5kb level
The correlation between expected rates and observed rates is probably weak. This is consistent across mutation types (the figure on the top) and the total rate counts (the bottom figure).
![](assets/exp_vs_obs_mut_type.png)
![](assets/exp_vs_obs_total.png)

#### Correlation at chromosome level
There is strong correlation when the mutation counts are summed up to the chromosome level.
![](assets/exp_vs_obs_chr_level.png)

#### Total counts for each mutation type
The total counts are also quite close between observed and expected.
```{r, echo=F}
table <- read.csv2("data/mut_count_total.csv", sep=",")
names(table) <- c("mutation type", "observed counts", "expected counts")
knitr::kable(table, digits = 1)
```
![](assets/mut_type_total.png)


### Definition of hotspots
$\ell(\alpha;y_i,\mu_i)$ is actually the normalization factor for $P(\theta|y_i,\mu_i,\alpha)$, the posterior $theta$ is $Gamma(y_i+\alpha, \mu_i+\alpha)$. Hence we have
$$
E(\theta|y_i,\mu_i,\alpha)=\frac{y_i+\alpha}{\mu_i+\alpha}
$$

Previously, I tried to define hotspots based on $E(\theta|y_i,\mu_i,\alpha)$. As shown in the figure below, large $\alpha$ in some mutation types may make it difficult to find hotspots.
![](assets/theta_post_mut_type.png)

Alternatively, we detect hotspots with a simple test. Say the null model is that:
$$
y_i \sim Poisson(\mu_i)
$$
And the alternative is that $y_i$ is not sampled from the Poisson of $\mu_i$. We used $P<0.001$ as the criteria for 5k windows. The cutoff was 0.05 for 100k windows.

### Hotspots continuity
I computed how many hotspot windows are actually adjacent to another ("continuous" column in the count table). C_to_T and C_to_G have the most hotspots.
```{r, echo=F}
table <- read.csv("data/hotspot_stats.csv",sep=",", header=1, row.names=1)
knitr::kable(table)
```

Below is the statistics of hotspots on 20K base levels.
```{r, echo=F}
table <- read.csv("data/hotspot_stats_20k.csv",sep=",", header=1, row.names=1)
knitr::kable(table)
```


When mutation counts are aggregated to 100K base levels, somehow mutations with the most hotspots become "A_to..." mutation types.  
```{r, echo=F}
table <- read.csv("data/hotspot_stats_100k.csv",sep=",", header=1, row.names=1)
knitr::kable(table)
```





Then, we looked into the distance from each hotspot to its closest hotspot window. Hotspots in C_to_T and A_to_G have smaller distances. This result is consistent with the count table above
![](assets/hotspot_dist.png). 

![](assets/hotspot_dist_20k.png)

![](assets/hotspot_dist_100k.png)

### Hotspot overlap across mutation types (one 5k window away)
Here, we define the overlap as two hotspot windows are one window away. Jaccard coefficient was computed.
$$
Jaccard(A,B) = \dfrac{|A\cap B|}{|A\cup B|}
$$

```{r, echo=F, message=F, warning=F}
library(rtracklayer)
library(ggplot2)
beds <- list.files("../controls_5k", full.names = T, pattern="hotspot")
bed.ranges <- list()
for (i in 1:length(beds)) {
  temp <- vroom::vroom(beds[i], col_names =T, show_col_types = F)
  bed.ranges[[i]] <- makeGRangesFromDataFrame(temp, keep.extra.columns = T, 
                                   start.field = "loc", end.field = "loc")
  start(bed.ranges[[i]]) <- start(bed.ranges[[i]]) - 1
  end(bed.ranges[[i]]) <- end(bed.ranges[[i]]) + 1
}

names <- sapply(strsplit(list.files("../controls_5k", pattern="hotspot"),"\\."), function(x) {x[1]})
names <- substr(names, 1, nchar(names)-11)
overlap <- expand.grid(x=names, y=names)
overlap$Jaccard <- 0
for (i in 1:length(beds)) {
  overlap$Jaccard[((i-1)*9+1):(i*9)] <- sapply(bed.ranges, function(x) {length(GenomicRanges::intersect(x, bed.ranges[[i]]))/length(GenomicRanges::union(x, bed.ranges[[i]]))})
  overlap$Jaccard[(i-1)*9+i] <- 0
}
ggplot(overlap, aes(x, y, fill= Jaccard)) + geom_tile() + theme(axis.text.x=element_text(angle = 45, hjust = 1))

```

```{r, echo=F, message=F, warning=F}
library(rtracklayer)
library(ggplot2)
beds <- list.files("data/hotspot_20k", full.names = T, pattern="hotspot")
bed.ranges <- list()
for (i in 1:length(beds)) {
  temp <- vroom::vroom(beds[i], col_names =T, show_col_types = F)
  bed.ranges[[i]] <- makeGRangesFromDataFrame(temp, keep.extra.columns = T,
                                   start.field = "loc", end.field = "loc")
  start(bed.ranges[[i]]) <- start(bed.ranges[[i]]) - 1
  end(bed.ranges[[i]]) <- end(bed.ranges[[i]]) + 1
}

names <- sapply(strsplit(list.files("data/hotspot_20k", pattern="hotspot"),"\\."), function(x) {x[1]})
names <- substr(names, 1, nchar(names)-8)
overlap <- expand.grid(x=names, y=names)
overlap$Jaccard <- 0
for (i in 1:length(beds)) {
  overlap$Jaccard[((i-1)*9+1):(i*9)] <- sapply(bed.ranges, function(x) {length(GenomicRanges::intersect(x, bed.ranges[[i]]))/length(GenomicRanges::union(x, bed.ranges[[i]]))})
  overlap$Jaccard[(i-1)*9+i] <- 0
}
ggplot(overlap, aes(x, y, fill= Jaccard)) + geom_tile() + theme(axis.text.x=element_text(angle = 45, hjust = 1))

```



```{r, echo=F, message=F, warning=F}
library(rtracklayer)
library(ggplot2)
beds <- list.files("data/hotspot_100k", full.names = T, pattern="hotspot")
bed.ranges <- list()
for (i in 1:length(beds)) {
  temp <- vroom::vroom(beds[i], col_names =T, show_col_types = F)
  bed.ranges[[i]] <- makeGRangesFromDataFrame(temp, keep.extra.columns = T,
                                   start.field = "loc", end.field = "loc")
  start(bed.ranges[[i]]) <- start(bed.ranges[[i]]) - 1
  end(bed.ranges[[i]]) <- end(bed.ranges[[i]]) + 1
}

names <- sapply(strsplit(list.files("data/hotspot_100k", pattern="hotspot"),"\\."), function(x) {x[1]})
names <- substr(names, 1, nchar(names)-5)
overlap <- expand.grid(x=names, y=names)
overlap$Jaccard <- 0
for (i in 1:length(beds)) {
  overlap$Jaccard[((i-1)*9+1):(i*9)] <- sapply(bed.ranges, function(x) {length(GenomicRanges::intersect(x, bed.ranges[[i]]))/length(GenomicRanges::union(x, bed.ranges[[i]]))})
  overlap$Jaccard[(i-1)*9+i] <- 0
}
ggplot(overlap, aes(x, y, fill= Jaccard)) + geom_tile() + theme(axis.text.x=element_text(angle = 45, hjust = 1))

```



```{r, echo=F, message=F}
library(ChIPpeakAnno)

res <- makeVennDiagram(Peaks=bed.ranges[c(2,8,9)], NameOfPeaks=names[c(2,8,9)], fill=c("red","blue","yellow"))
```


## Bayesian Poisson regression
Given observed mutation rates ($y_i$), baseline mutation rates ($\mu_i$) and 14 genomic features ($X_i$). The generative process is

$$
\tau^2_j \sim Inv-\chi^2(1,1)
$$
$$
\beta_{jt} \sim N(0,\tau^2_j)
$$
$$
\theta_{it} = exp(\beta_{0t}+\sum_j \beta_{jt}X_{ij})
$$

$$
Y_{it} \sim Pois(\mu_{it} \cdot \theta_{it})
$$

### Numpyro MCMC was used to infer the posterior of $\beta_{jt}$. 

I initialized four chains. For each chain, 2000 samples were drawn after warmup of 1000 samples. Chains seem mixed.

![](assets/numpyro_mixing.png)



![](assets/beta_jt_numpyro.png). 

### Stan implementation
The same model was implemented with rstan. The NUTS sampler was used. I initiated 4 chains. Within each chain, 1000 samples were drawn after warm up of 1000.

#### Simulation
In the simulation experiment, $\tau$ and $\beta$ are generated according to the generative process above. As for the genomic features ($X$), we tested with randomly generated features and the real data features.

** Randomly genearted features
All the features are sampled from the Normal distributions. Baseline mutation rates are sampled from uniform distribution within $[0,1]$. The diagnostics about the energy distribution and the divergent transition is shown in the figures below. The energy distribution is quite close to each other.

![](assets/simulation/beta_simulation_energyplot.png)
As for the pairs plot, note that the crossover between the same two parameters represent different half chains. If there are divergent transitions, dots will become red cross.
![](assets/simulation/beta_simulation_pairplot.png)

Shown in the figure below, the estimate is very close to the ground truth. The plum pillar is ground truth and the blue dots and blue lines are the estimates.

![](assets/simulation/simulation_beta_stan.png)

**Real genomic features**
With real genomic features and real baseline, somehow the synthetic mutation count ($y$) is quite different to the real ones. We proceeded to perform inference and the sampler is having difficulty drawing representative samples. As shown in the figure below, $\pi|q$ is more centered than the marginal $\pi$.

![](assets/simulation/beta_real_baseline_energy.png)

For the pairs plots, there are a few red crosses indicating divergent transitions. And $\tau[1]$ in the first row of scatter plot has a wider range.

![](assets/simulation/beta_real_baseline_pairplot.png)

And we observed that most estimates of $beta$ have high uncertainty and far away from the ground truth.
![](assets/simulation/simulation_beta_stan_real_baseline.png)


#### Diagnostics for real dataset
First, the four chains were well mixed, as shown below.

![](assets/stan_trace.png)

Next, the energy distribution was close to the marginal energy distribution. According to Stan user guide, this is a good sign that the sampler explored the target distribution efficiently.

![](assets/stan_diag_energy.png)

We further performed the posterior predictive check. Essentially, we draw a vector of $\beta$ from the posterior and draw $\tilde{y}$ from the Poisson distribution parameterized by $\beta$. Then we compared the distribution of $\tilde{y}$ and observed $y$.

As shown in the figure below,  $\tilde{y}$ is close to $y$ across mutation types.
![](assets/post_pred_checks.png)


#### Parameter estimation for real dataset
![](assets/stan_beta_est.png)

#### Cross validation with log pointwise posterior probability (lpd)
The idea is to estimate parameters with one set of windows and estimate the fitness in another set of windows. According to Stan user guide, lpd can be computed as:

$$
lpd=\frac{1}{N}\sum_{n=1}^N{log\,p(y_n|y,M_1)}
$$
where $p(y_n|y,M_1)$ can be obtained by averaging over posterior predictive.
$$
p(y_n|y,M_1)=\int{P(y_n|\theta)P(\theta|y,M_1)}\,d\theta=E_{\theta|y}(l(\theta;y_n))
$$


Suppose we already obtained posterior samples of $\tau$ and $\beta$ from the odd windows. To estimate lpd in the even windows, 




